# Model to use for Fine-tuning
model_name: "gpt2-small (124M)"

model_configs:
  "gpt2-small (124M)":
    vocab_size: 50257
    context_length: 1024
    embedding_dim: 768
    num_layers: 12
    num_heads: 12
    drop_rate: 0.1
    "qkv_bias": True
  "gpt2-medium (355M)":
    vocab_size: 50257
    context_length: 1024
    embedding_dim: 1024
    num_layers: 24
    num_heads: 16
    drop_rate: 0.1
    "qkv_bias": True
  "gpt2-large (774M)":
    vocab_size: 50257
    context_length: 1024
    embedding_dim: 1280
    num_layers: 36
    num_heads: 20
    drop_rate: 0.1
    "qkv_bias": True
  "gpt2-xl (1558M)":
    vocab_size: 50257
    context_length: 1024
    embedding_dim: 1600
    num_layers: 48
    num_heads: 25
    drop_rate: 0.1
    "qkv_bias": True