model_configs:
  "gpt2-small (124M)":
    vocab_size: 50257
    context_length: 1024
    emb_dim: 768
    n_layers: 12
    n_heads: 12
    drop_rate: 0.1
    "qkv_bias": True
  "gpt2-medium (355M)":
    vocab_size: 50257
    context_length: 1024
    emb_dim: 1024
    n_layers: 24
    n_heads: 16
    drop_rate: 0.1
    "qkv_bias": True
  "gpt2-large (774M)":
    vocab_size: 50257
    context_length: 1024
    emb_dim: 1280
    n_layers: 36
    n_heads: 20
    drop_rate: 0.1
    "qkv_bias": True
  "gpt2-xl (1558M)":
    vocab_size: 50257
    context_length: 1024
    emb_dim: 1600
    n_layers: 48
    n_heads: 25
    drop_rate: 0.1
    "qkv_bias": True