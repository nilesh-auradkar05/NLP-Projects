{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-07-31 22:20:26.281618: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754014826.359482    5661 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754014826.381467    5661 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754014826.527953    5661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754014826.527989    5661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754014826.527993    5661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754014826.527996    5661 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-31 22:20:26.546708: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# System imports\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from typing import Dict, Any, Optional, Union\n",
    "from pathlib import Path\n",
    "import psutil\n",
    "from datetime import datetime\n",
    "from dotenv import load_dotenv\n",
    "import regex as re\n",
    "\n",
    "# External imports\n",
    "from tqdm.auto import tqdm\n",
    "import yaml\n",
    "from loguru import logger\n",
    "import tiktoken\n",
    "import wandb\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, Callback, EarlyStopping\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mnilesh-auradkar\u001b[0m (\u001b[33mnilesh-auradkar-personal\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "wandb.login(key=os.getenv(\"WANDB_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script downloads the GPT-2 model weights and loads them into a dictionary.\n",
    "\"\"\"\n",
    "\n",
    "def download_and_load_gpt2(model_size=\"124M\", models_dir=\"../model_weights/\"):\n",
    "    # Validate model size\n",
    "    allowed_sizes = (\"124M\", \"355M\", \"774M\", \"1558M\")\n",
    "    if model_size not in allowed_sizes:\n",
    "        raise ValueError(f\"Model size not in {allowed_sizes}\")\n",
    "\n",
    "    # Define paths\n",
    "    model_dir = os.path.join(models_dir, model_size)\n",
    "    base_url = \"https://openaipublic.blob.core.windows.net/gpt-2/models\"\n",
    "    filenames = [\n",
    "        \"checkpoint\", \"encoder.json\", \"hparams.json\",\n",
    "        \"model.ckpt.data-00000-of-00001\", \"model.ckpt.index\",\n",
    "        \"model.ckpt.meta\", \"vocab.bpe\"\n",
    "    ]\n",
    "\n",
    "    print(f\"Downloading GPT-2 {model_size} model to {model_dir}\")\n",
    "    \n",
    "    # Download files\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    for filename in filenames:\n",
    "        file_url = f\"{base_url}/{model_size}/{filename}\"\n",
    "        file_path = os.path.join(model_dir, filename)\n",
    "        download_file(file_url, file_path)\n",
    "\n",
    "    print(\"Download completed. Loading model parameters...\")\n",
    "\n",
    "    # Load settings and params\n",
    "    tf_ckpt_path = tf.train.latest_checkpoint(model_dir)\n",
    "    settings = json.load(open(os.path.join(model_dir, \"hparams.json\")))\n",
    "    params = load_gpt2_params_from_tf_ckpt(tf_ckpt_path, settings)\n",
    "\n",
    "    print(\"Model loaded successfully!\")\n",
    "    print(f\"Model configuration: {settings}\")\n",
    "    \n",
    "    return settings, params\n",
    "\n",
    "def download_file(url, destination):\n",
    "    try:\n",
    "        # Send a GET request to download the file, disabling SSL verification\n",
    "        response = requests.get(url, stream=True, verify=False)\n",
    "        response.raise_for_status()  # Raise an exception for bad status codes\n",
    "\n",
    "        # Get the total file size from headers, defaulting to 0 if not present\n",
    "        file_size = int(response.headers.get(\"content-length\", 0))\n",
    "\n",
    "        # Check if file exists and has the same size\n",
    "        if os.path.exists(destination):\n",
    "            file_size_local = os.path.getsize(destination)\n",
    "            if file_size == file_size_local and file_size > 0:\n",
    "                print(f\"File already exists and is up-to-date: {destination}\")\n",
    "                return\n",
    "\n",
    "        # Define the block size for reading the file\n",
    "        block_size = 1024  # 1 Kilobyte\n",
    "\n",
    "        # Initialize the progress bar with total file size\n",
    "        progress_bar_description = url.split(\"/\")[-1]  # Extract filename from URL\n",
    "        with tqdm(total=file_size, unit=\"iB\", unit_scale=True, desc=progress_bar_description) as progress_bar:\n",
    "            # Open the destination file in binary write mode\n",
    "            with open(destination, \"wb\") as file:\n",
    "                # Iterate over the file data in chunks\n",
    "                for chunk in response.iter_content(block_size):\n",
    "                    if chunk:  # Filter out keep-alive chunks\n",
    "                        progress_bar.update(len(chunk))  # Update progress bar\n",
    "                        file.write(chunk)  # Write the chunk to the file\n",
    "\n",
    "        print(f\"Downloaded: {destination}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading the file: {e}\")\n",
    "        print(f\"Please check the URL: {url}\")\n",
    "        raise\n",
    "\n",
    "def load_gpt2_params_from_tf_ckpt(ckpt_path, settings):\n",
    "    # Initialize parameters dictionary with empty blocks for each layer\n",
    "    params = {\"blocks\": [{} for _ in range(settings[\"n_layer\"])]}\n",
    "\n",
    "    # Iterate over each variable in the checkpoint\n",
    "    for name, _ in tf.train.list_variables(ckpt_path):\n",
    "        # Load the variable and remove singleton dimensions\n",
    "        variable_array = np.squeeze(tf.train.load_variable(ckpt_path, name))\n",
    "\n",
    "        # Process the variable name to extract relevant parts\n",
    "        variable_name_parts = name.split(\"/\")[1:]  # Skip the 'model/' prefix\n",
    "\n",
    "        # Identify the target dictionary for the variable\n",
    "        target_dict = params\n",
    "        if variable_name_parts[0].startswith(\"h\"):\n",
    "            layer_number = int(variable_name_parts[0][1:])\n",
    "            target_dict = params[\"blocks\"][layer_number]\n",
    "\n",
    "        # Recursively access or create nested dictionaries\n",
    "        for key in variable_name_parts[1:-1]:\n",
    "            target_dict = target_dict.setdefault(key, {})\n",
    "\n",
    "        # Assign the variable array to the last key\n",
    "        last_key = variable_name_parts[-1]\n",
    "        target_dict[last_key] = variable_array\n",
    "\n",
    "    return params\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     try:\n",
    "#         # Download and load GPT-2 model\n",
    "#         settings, params = download_and_load_gpt2(model_size=\"124M\")\n",
    "        \n",
    "#         # Print some basic info about the loaded model\n",
    "#         print(\"\\nModel details:\")\n",
    "#         print(f\"- Vocabulary size: {settings.get('n_vocab', 'Unknown')}\")\n",
    "#         print(f\"- Number of layers: {settings.get('n_layer', 'Unknown')}\")\n",
    "#         print(f\"- Number of attention heads: {settings.get('n_head', 'Unknown')}\")\n",
    "#         print(f\"- Embedding dimension: {settings.get('n_embd', 'Unknown')}\")\n",
    "#         print(f\"- Context length: {settings.get('n_ctx', 'Unknown')}\")\n",
    "        \n",
    "#     except Exception as e:\n",
    "#         print(f\"Error: {e}\")\n",
    "#         print(\"Make sure you have the required dependencies installed:\")\n",
    "#         print(\"pip install tensorflow requests tqdm numpy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, emb_dim):\n",
    "        super().__init__()\n",
    "        self.eps = 1e-5\n",
    "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
    "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim=-1, keepdim=True)\n",
    "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
    "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
    "        return self.scale * norm_x + self.shift\n",
    "\n",
    "class GELU(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return 0.5 * x * (1 + torch.tanh(\n",
    "            torch.sqrt(torch.tensor(2.0 / torch.pi)) * \n",
    "            (x + 0.044715 * torch.pow(x, 3))\n",
    "        ))\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(cfg[\"embedding_dim\"], 4 * cfg[\"embedding_dim\"]),\n",
    "            GELU(),\n",
    "            nn.Linear(4 * cfg[\"embedding_dim\"], cfg[\"embedding_dim\"]),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
    "        super().__init__()\n",
    "        assert (d_out % num_heads == 0), \\\n",
    "            \"d_out must be divisible by num_heads\"\n",
    "\n",
    "        self.d_out = d_out\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = d_out // num_heads # Reduce the projection dim to match desired output dim\n",
    "\n",
    "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
    "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.register_buffer(\n",
    "            \"mask\",\n",
    "            torch.triu(torch.ones(context_length, context_length),\n",
    "                       diagonal=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, num_tokens, d_in = x.shape\n",
    "\n",
    "        keys = self.W_key(x) # Shape: (b, num_tokens, d_out)\n",
    "        queries = self.W_query(x)\n",
    "        values = self.W_value(x)\n",
    "\n",
    "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
    "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim) \n",
    "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
    "\n",
    "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
    "        keys = keys.transpose(1, 2)\n",
    "        queries = queries.transpose(1, 2)\n",
    "        values = values.transpose(1, 2)\n",
    "\n",
    "        # Compute scaled dot-product attention/self-attention\n",
    "        attn_scores = queries @ keys.transpose(2, 3)\n",
    "\n",
    "        # Original mask truncated to the number of tokens and converted to boolean\n",
    "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
    "\n",
    "        # Use the mask to fill attention scores\n",
    "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
    "        \n",
    "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "\n",
    "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
    "        context_vec = (attn_weights @ values).transpose(1, 2) \n",
    "        \n",
    "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
    "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
    "        context_vec = self.out_proj(context_vec)\n",
    "\n",
    "        return context_vec\n",
    "    \n",
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.mask_attn = MultiHeadAttention(\n",
    "            d_in=cfg[\"embedding_dim\"],\n",
    "            d_out=cfg[\"embedding_dim\"],\n",
    "            context_length=cfg[\"context_length\"],\n",
    "            num_heads=cfg[\"num_heads\"], \n",
    "            dropout=cfg[\"drop_rate\"],\n",
    "            qkv_bias=cfg[\"qkv_bias\"])\n",
    "        self.ffn_block = FeedForward(cfg)\n",
    "        self.norm_1 = LayerNorm(cfg[\"embedding_dim\"])\n",
    "        self.norm_2 = LayerNorm(cfg[\"embedding_dim\"])\n",
    "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Shortcut connection for attention block\n",
    "        shortcut = x\n",
    "        x = self.norm_1(x)\n",
    "        x = self.mask_attn(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut  # Add the original input back\n",
    "\n",
    "        # Shortcut connection for feed forward block\n",
    "        shortcut = x\n",
    "        x = self.norm_2(x)\n",
    "        x = self.ffn_block(x)\n",
    "        x = self.drop_shortcut(x)\n",
    "        x = x + shortcut\n",
    "\n",
    "        return x\n",
    "        # 2*4*768\n",
    "\n",
    "class GPT2ModelClone(nn.Module):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__()\n",
    "        self.tok_embeddings = nn.Embedding(cfg[\"vocab_size\"], cfg[\"embedding_dim\"])\n",
    "        self.pos_embeddings = nn.Embedding(cfg[\"context_length\"], cfg[\"embedding_dim\"])\n",
    "        self.drop_embeddings = nn.Dropout(cfg[\"drop_rate\"])\n",
    "        \n",
    "        self.transformer_blocks = nn.Sequential(\n",
    "            *[TransformerBlock(cfg) for _ in range(cfg[\"num_layers\"])])\n",
    "        \n",
    "        self.final_norm = LayerNorm(cfg[\"embedding_dim\"])\n",
    "        self.out_head = nn.Linear(\n",
    "            cfg[\"embedding_dim\"], cfg[\"vocab_size\"], bias=False\n",
    "        )\n",
    "\n",
    "    def forward(self, in_idx):\n",
    "        batch_size, seq_len = in_idx.shape\n",
    "        tok_embeds = self.tok_embeddings(in_idx)\n",
    "        pos_embeds = self.pos_embeddings(torch.arange(seq_len, device=in_idx.device))\n",
    "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
    "        x = self.drop_embeddings(x)\n",
    "        x = self.transformer_blocks(x)\n",
    "        x = self.final_norm(x)\n",
    "        logits = self.out_head(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoadModelConfig:\n",
    "    def __init__(self, yaml_file: Union[str, Path] = \"../config/model_config.yaml\"):\n",
    "        \"\"\"\n",
    "        Inititalize the model config class\n",
    "\n",
    "        Args:\n",
    "            yaml_file: Path to the YAML configuration file\n",
    "        \"\"\"\n",
    "        if yaml_file is None:\n",
    "            yaml_file = Path(__file__).parent.parent / \"config\" / \"model_config.yaml\"\n",
    "\n",
    "        self.yaml_file = Path(yaml_file)\n",
    "        self.config_data = self.load_yaml()\n",
    "\n",
    "    def load_yaml(self) -> Dict:\n",
    "        \"\"\"Load the YAML configuration file.\"\"\"\n",
    "        try:\n",
    "            if not self.yaml_file.exists():\n",
    "                raise FileNotFoundError(f\"The file {self.yaml_file} does not exist.\")\n",
    "            \n",
    "            with self.yaml_file.open(\"r\", encoding=\"utf-8\") as f:\n",
    "                data = yaml.safe_load(f)\n",
    "                return data.get(\"model_configs\", {})\n",
    "        except yaml.YAMLError as e:\n",
    "            raise ValueError(f\"Error parsing YAML file: {e}\")\n",
    "        except Exception as e:\n",
    "            raise IOError(f\"Error reading {self.yaml_file}: {e}\")\n",
    "        \n",
    "    def list_all_models(self) -> list:\n",
    "        \"\"\"Return a list of all model keys.\"\"\"\n",
    "        return list(self.config_data.keys())\n",
    "    \n",
    "    def get_model_config(self, model_name: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Extract the model configuration for a given model name.\n",
    "\n",
    "        Args:\n",
    "            model_name: The name of the model to extract configuration for.\n",
    "\n",
    "        Returns:\n",
    "            Model Configuration dictionary or None if the model name is not found.\n",
    "        \"\"\"\n",
    "        return self.config_data.get(model_name, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrepareModelWithPreTrainedWeights:\n",
    "    def __init__(self, model_name: str, device: str = \"cuda\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self._load_all()\n",
    "\n",
    "    def _load_all(self):\n",
    "        self.settings, self.params = self._get_settings_and_params()\n",
    "        self.model_config = self._get_model_config()\n",
    "        # print(f\"Self.model_config: {self.model_config}\")\n",
    "        self.model = GPT2ModelClone(self.model_config)\n",
    "        self.model.eval()\n",
    "        self._load_gpt2_weights_into_model()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "    def _assign_params(self, left, right):\n",
    "        if left.shape != right.shape:\n",
    "            raise ValueError(f\"Shape mismatch: {left.shape} != {right.shape}\")\n",
    "        return torch.nn.Parameter(torch.tensor(right))\n",
    "\n",
    "    def _get_settings_and_params(self):\n",
    "        settings, params = download_and_load_gpt2(model_size=\"124M\", models_dir=\"./model_weights/\")\n",
    "        print(f\"Settings: {settings}\")\n",
    "        print(f\"Params: {params.keys()}\")\n",
    "        return settings, params\n",
    "\n",
    "    def _get_model_config(self):\n",
    "        config = LoadModelConfig()\n",
    "        print(config.list_all_models())\n",
    "        model_config = config.get_model_config(model_name=self.model_name)\n",
    "        print(f\"Returned Model config for {self.model_name}: {model_config}\")\n",
    "        return model_config\n",
    "\n",
    "    def _load_gpt2_weights_into_model(self):\n",
    "        self.model.pos_embeddings.weight = self._assign_params(self.model.pos_embeddings.weight, self.params[\"wpe\"])\n",
    "        self.model.tok_embeddings.weight = self._assign_params(self.model.tok_embeddings.weight, self.params[\"wte\"])\n",
    "\n",
    "        for block in range(len(self.params[\"blocks\"])):\n",
    "            # Load the weights for Query, key and value\n",
    "            q_w, k_w, v_w = np.split(\n",
    "                (self.params[\"blocks\"][block][\"attn\"][\"c_attn\"])[\"w\"], 3, axis=-1)\n",
    "            self.model.transformer_blocks[block].mask_attn.W_query.weight = self._assign_params(\n",
    "                self.model.transformer_blocks[block].mask_attn.W_query.weight, q_w.T)\n",
    "            self.model.transformer_blocks[block].mask_attn.W_key.weight = self._assign_params(\n",
    "                self.model.transformer_blocks[block].mask_attn.W_key.weight, k_w.T)\n",
    "            self.model.transformer_blocks[block].mask_attn.W_value.weight = self._assign_params(\n",
    "                self.model.transformer_blocks[block].mask_attn.W_value.weight, v_w.T)\n",
    "\n",
    "            # Load the weights for bias\n",
    "            q_b, k_b, v_b = np.split(\n",
    "                (self.params[\"blocks\"][block][\"attn\"][\"c_attn\"])[\"b\"], 3, axis=-1)\n",
    "            self.model.transformer_blocks[block].mask_attn.W_query.bias = self._assign_params(\n",
    "                self.model.transformer_blocks[block].mask_attn.W_query.bias, q_b)\n",
    "            self.model.transformer_blocks[block].mask_attn.W_key.bias = self._assign_params(\n",
    "                self.model.transformer_blocks[block].mask_attn.W_key.bias, k_b)\n",
    "            self.model.transformer_blocks[block].mask_attn.W_value.bias = self._assign_params(\n",
    "                self.model.transformer_blocks[block].mask_attn.W_value.bias, v_b)\n",
    "            \n",
    "            # Load output layer weights\n",
    "            self.model.transformer_blocks[block].mask_attn.out_proj.weight = self._assign_params(\n",
    "                self.model.transformer_blocks[block].mask_attn.out_proj.weight,\n",
    "                self.params[\"blocks\"][block][\"attn\"][\"c_proj\"][\"w\"].T)\n",
    "            self.model.transformer_blocks[block].mask_attn.out_proj.bias = self._assign_params(\n",
    "                self.model.transformer_blocks[block].mask_attn.out_proj.bias,\n",
    "                self.params[\"blocks\"][block][\"attn\"][\"c_proj\"][\"b\"])\n",
    "            \n",
    "            # Load the weights for feed forward block\n",
    "            self.model.transformer_blocks[block].ffn_block.layers[0].weight = self._assign_params(\n",
    "                self.model.transformer_blocks[block].ffn_block.layers[0].weight,\n",
    "                self.params[\"blocks\"][block][\"mlp\"][\"c_fc\"][\"w\"].T)\n",
    "            self.model.transformer_blocks[block].ffn_block.layers[0].bias = self._assign_params(\n",
    "                self.model.transformer_blocks[block].ffn_block.layers[0].bias,\n",
    "                self.params[\"blocks\"][block][\"mlp\"][\"c_fc\"][\"b\"])\n",
    "            self.model.transformer_blocks[block].ffn_block.layers[2].weight = self._assign_params(\n",
    "                self.model.transformer_blocks[block].ffn_block.layers[2].weight,\n",
    "                self.params[\"blocks\"][block][\"mlp\"][\"c_proj\"][\"w\"].T)\n",
    "            self.model.transformer_blocks[block].ffn_block.layers[2].bias = self._assign_params(\n",
    "                self.model.transformer_blocks[block].ffn_block.layers[2].bias,\n",
    "                self.params[\"blocks\"][block][\"mlp\"][\"c_proj\"][\"b\"])\n",
    "\n",
    "            # Load Normalization weights\n",
    "            self.model.transformer_blocks[block].norm_1.scale = self._assign_params(\n",
    "                self.model.transformer_blocks[block].norm_1.scale,\n",
    "                self.params[\"blocks\"][block][\"ln_1\"][\"g\"])\n",
    "            self.model.transformer_blocks[block].norm_1.shift = self._assign_params(\n",
    "                self.model.transformer_blocks[block].norm_1.shift,\n",
    "                self.params[\"blocks\"][block][\"ln_1\"][\"b\"])\n",
    "            self.model.transformer_blocks[block].norm_2.scale = self._assign_params(\n",
    "                self.model.transformer_blocks[block].norm_2.scale,\n",
    "                self.params[\"blocks\"][block][\"ln_2\"][\"g\"])\n",
    "            self.model.transformer_blocks[block].norm_2.shift = self._assign_params(\n",
    "                self.model.transformer_blocks[block].norm_2.shift,\n",
    "                self.params[\"blocks\"][block][\"ln_2\"][\"b\"])\n",
    "            \n",
    "        self.model.final_norm.scale = self._assign_params(self.model.final_norm.scale, self.params[\"g\"])\n",
    "        self.model.final_norm.shift = self._assign_params(self.model.final_norm.shift, self.params[\"b\"])\n",
    "        self.model.out_head.weight = self._assign_params(self.model.out_head.weight, self.params[\"wte\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedBillSumDatasetNew(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length, min_summary_length=1):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.processed_data = []\n",
    "        self.min_summary_length = min_summary_length\n",
    "        self.dataset = dataset\n",
    "        self.pad_token_id = tokenizer.eot_token\n",
    "\n",
    "        logger.info(\"Processng and masking dataset for summarization...\")\n",
    "        logger.info(f\"Original Dataset Size: {len(self.dataset)}\")\n",
    "        skipped_count = 0\n",
    "        truncate_stats = {\n",
    "            \"full\": 0,\n",
    "            \"section_trunc\": 0,\n",
    "            \"token_trunc\": 0,\n",
    "            \"sliding_window\": 0,\n",
    "        }\n",
    "\n",
    "        for idx, sample in tqdm(enumerate(self.dataset)):\n",
    "            try:\n",
    "\n",
    "                if not sample.get(\"text\") or not sample.get(\"summary\"):\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                article_text = str(sample.get(\"text\")).strip()\n",
    "                summary_text = str(sample.get(\"summary\")).strip()\n",
    "\n",
    "                if len(article_text) <= 50 or len(summary_text) < self.min_summary_length:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                clean_text = self._remove_boilerplate(article_text)\n",
    "\n",
    "                summary_str = f\"SUMMARY: {summary_text}\"\n",
    "                summary_tokens = self.tokenizer.encode(\n",
    "                    summary_str,\n",
    "                    allowed_special=\"all\",\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                required_summary_space = len(summary_tokens) + 1\n",
    "                if required_summary_space > self.max_length - 50:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                available_article_tokens = self.max_length - required_summary_space\n",
    "\n",
    "                article_str = f\"ARTICLE: {clean_text}\"\n",
    "                article_tokens = self.tokenizer.encode(\n",
    "                    article_str,\n",
    "                    allowed_special=\"all\",\n",
    "                    add_special_tokens=False,\n",
    "                )\n",
    "\n",
    "                if len(article_tokens) <= available_article_tokens:\n",
    "                    truncate_stats[\"full\"] += 1\n",
    "                else:\n",
    "                    truncated_text = self._truncate_sections(\n",
    "                        clean_text,\n",
    "                        available_article_tokens,\n",
    "                    )\n",
    "\n",
    "                    if not truncated_text or len(truncated_text) < 50:\n",
    "                        truncated_text = self._token_level_truncate(\n",
    "                            clean_text,\n",
    "                            available_article_tokens,\n",
    "                        )\n",
    "                        truncate_stats[\"token_trunc\"] += 1\n",
    "                    else:\n",
    "                        truncate_stats[\"section_trunc\"] += 1\n",
    "\n",
    "                    article_str = f\"ARTICLE: {truncated_text}\"\n",
    "                    article_tokens = self.tokenizer.encode(\n",
    "                        article_str,\n",
    "                        allowed_special=\"all\",\n",
    "                        add_special_tokens=False,\n",
    "                    )[:available_article_tokens]\n",
    "\n",
    "                if len(article_tokens) < 50:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "\n",
    "                input_ids = article_tokens + summary_tokens + self.tokenizer.convert_tokens_to_ids(self.tokenizer.eot_token)\n",
    "                labels = [-100] * len(article_tokens) + summary_tokens + [self.tokenizer.convert_tokens_to_ids(self.tokenizer.eot_token)]\n",
    "\n",
    "                if len(input_ids) > self.max_length:\n",
    "                    input_ids = input_ids[:self.max_length]\n",
    "                    labels = labels[:self.max_length]\n",
    "\n",
    "                padding_length = self.max_length - len(input_ids)\n",
    "                if padding_length > 0:\n",
    "                    input_ids += [self.pad_token_id] * padding_length\n",
    "                    labels += [-100] * padding_length\n",
    "\n",
    "                self.processed_data.append({\n",
    "                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                    \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "                })\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing samples: {idx}: {str(e)}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "        \n",
    "        total_samples = len(self.dataset)\n",
    "        logger.info(f\"Processed {len(self.processed_data)}/{total_samples} samples\")\n",
    "        logger.info(f\"Skipped: {skipped_count} | Truncated: {truncate_stats}\")\n",
    "        logger.info(f\"Section trunc: {truncate_stats['section_trunc']} | Token trunc: {truncate_stats['token_trunc']}\")\n",
    "\n",
    "    def _remove_boilerplate(self, text):\n",
    "        \"\"\"Remove legal boilerplate from the billsum dataset.\"\"\"\n",
    "        text = re.sub(\n",
    "            r'(Be it enacted by the Senate and House of Representatives|'\n",
    "            r'The Congress of the United States.*?)(?=\\n\\s*SECTION\\s+\\d)',\n",
    "            '',\n",
    "            text,\n",
    "            flags=re.DOTALL | re.IGNORECASE\n",
    "        )\n",
    "        # Remove middle section.\n",
    "        # As per the paper, the middle section is not useful for summarization.\n",
    "        # The main information for summarization lies in the first 3-5 sections and the last sections.\n",
    "        text = re.sub(\n",
    "            r'\\nSECTION\\s+\\d+\\.\\s*\\(a\\)\\s*.*?(?=\\n\\s*SECTION|\\Z)',\n",
    "            '\\n',\n",
    "            text,\n",
    "            flags=re.DOTALL\n",
    "        )\n",
    "        return text.strip()\n",
    "    \n",
    "    def _truncate_sections(self, text, max_tokens):\n",
    "        \"\"\"Section truncation.\"\"\"\n",
    "        sections = re.split(\n",
    "            r'(\\n\\s*SECTION\\s+\\d+\\.?|\\n\\s*Sec\\.\\s+\\d+\\.?)',\n",
    "            text,\n",
    "            flags=re.IGNORECASE\n",
    "        )\n",
    "\n",
    "        if len(sections) <= 2:\n",
    "            return None\n",
    "\n",
    "        start_content = min(4, max(2, len(sections) // 4))\n",
    "        end_content = min(3, max(1, len(sections) // 5))\n",
    "\n",
    "        content = [sections[0]]\n",
    "        content += sections[1: start_content*2]\n",
    "        content += sections[-end_content*2:]\n",
    "\n",
    "        text = \"\".join(content).strip()\n",
    "\n",
    "        tokens = self.tokenizer.encode(\n",
    "            f\"ARTICLE: {text}\",\n",
    "            add_special_tokens=False,\n",
    "            allowed_special=\"all\",\n",
    "        )\n",
    "\n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "        \n",
    "        return self._reduce_sections(sections, max_tokens)\n",
    "    \n",
    "    def _reduce_sections(self, sections, max_tokens):\n",
    "        \"\"\"Progressively reduce sections until token limit is met\"\"\"\n",
    "        for front in range(min(4, len(sections)//3), 1, -1):\n",
    "            for back in range(min(3, len(sections)//4), 0, -1):\n",
    "                kept = [sections[0]] + sections[1:front*2] + sections[-back*2:]\n",
    "                candidate = ''.join(kept).strip()\n",
    "                \n",
    "                tokens = self.tokenizer.encode(\n",
    "                    f\"ARTICLE: {candidate}\", \n",
    "                    add_special_tokens=False,\n",
    "                    allowed_special=\"all\"\n",
    "                )\n",
    "                \n",
    "                if len(tokens) <= max_tokens:\n",
    "                    return candidate\n",
    "                    \n",
    "        # Final fallback: Minimal critical sections\n",
    "        candidate = sections[0] + sections[1] + sections[-2] + sections[-1]\n",
    "        return candidate.strip()\n",
    "    \n",
    "    def _token_level_truncation(self, text, max_tokens):\n",
    "        \"\"\"Fallback truncation with sentence boundary preservation\"\"\"\n",
    "        tokens = self.tokenizer.encode(\n",
    "            text, \n",
    "            add_special_tokens=False,\n",
    "            allowed_special=\"all\"\n",
    "        )\n",
    "        \n",
    "        if len(tokens) <= max_tokens:\n",
    "            return text\n",
    "            \n",
    "        # Preserve first 60% and last 30% with sentence boundaries\n",
    "        front_ratio = 0.65\n",
    "        front_tokens = tokens[:int(len(tokens) * front_ratio)]\n",
    "        back_tokens = tokens[-int(len(tokens) * 0.3):]\n",
    "        \n",
    "        # Find sentence boundaries\n",
    "        front_text = self.tokenizer.decode(front_tokens, skip_special_tokens=True)\n",
    "        last_sentence = re.search(r'[.!?]($|\\s)', front_text)\n",
    "        if last_sentence:\n",
    "            front_text = front_text[:last_sentence.end()]\n",
    "            front_tokens = self.tokenizer.encode(\n",
    "                front_text, \n",
    "                add_special_tokens=False,\n",
    "                allowed_special=\"all\"\n",
    "            )\n",
    "        \n",
    "        # Combine and validate\n",
    "        combined_tokens = front_tokens + back_tokens\n",
    "        if len(combined_tokens) > max_tokens:\n",
    "            combined_tokens = combined_tokens[:max_tokens]\n",
    "            \n",
    "        return self.tokenizer.decode(combined_tokens, skip_special_tokens=True)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedBillSumDataset(Dataset):\n",
    "    def __init__(self, dataset, tokenizer, max_length, min_summary_length=5):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.min_summary_length = min_summary_length\n",
    "        self.processed_data = []\n",
    "        self.dataset = dataset\n",
    "        self.pad_token_id = tokenizer.eot_token\n",
    "        \n",
    "        logger.info(\"Processing and masking dataset for summarization...\")\n",
    "        logger.info(f\"Original dataset size: {len(self.dataset)}\")\n",
    "        \n",
    "        skipped_count = 0\n",
    "        \n",
    "        for idx, sample in enumerate(tqdm(self.dataset)):\n",
    "            try:\n",
    "                # Basic validation\n",
    "                if not sample.get('text') or not sample.get('summary'):\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                article_text = str(sample['text']).strip()\n",
    "                summary_text = str(sample['summary']).strip()\n",
    "                \n",
    "                # Skip if either is too short\n",
    "                if len(article_text) < 10 or len(summary_text) < 10:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Create formatted input\n",
    "                article = f\"ARTICLE: {article_text} \"\n",
    "                summary = f\"SUMMARY: {summary_text}\"\n",
    "                \n",
    "                # Encode text\n",
    "                article_tokens = self.tokenizer.encode(article, allowed_special=\"all\")\n",
    "                summary_tokens = self.tokenizer.encode(summary, allowed_special=\"all\")\n",
    "                \n",
    "                # Add EOS token\n",
    "                eos_token = [self.tokenizer.eot_token]\n",
    "                \n",
    "                # Skip if summary is too short after tokenization\n",
    "                if len(summary_tokens) < self.min_summary_length:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Combine tokens\n",
    "                input_ids = article_tokens + summary_tokens + eos_token\n",
    "                \n",
    "                # Skip if too long even before padding\n",
    "                if len(input_ids) > self.max_length:\n",
    "                    # Try to truncate article but keep summary\n",
    "                    available_space = self.max_length - len(summary_tokens) - len(eos_token) - 50  # Buffer\n",
    "                    if available_space > 100:  # Need reasonable article length\n",
    "                        article_tokens = article_tokens[:available_space]\n",
    "                        input_ids = article_tokens + summary_tokens + eos_token\n",
    "                    else:\n",
    "                        skipped_count += 1\n",
    "                        continue\n",
    "                \n",
    "                # Create labels: mask article tokens, keep summary + eos tokens\n",
    "                labels = [-100] * len(article_tokens) + summary_tokens + eos_token\n",
    "                \n",
    "                # Pad to max_length\n",
    "                padding_length = self.max_length - len(input_ids)\n",
    "                if padding_length > 0:\n",
    "                    input_ids += [self.pad_token_id] * padding_length\n",
    "                    labels += [-100] * padding_length\n",
    "                \n",
    "                # Final validation - ensure we have target tokens\n",
    "                active_labels = [l for l in labels if l != -100]\n",
    "                if len(active_labels) < self.min_summary_length:\n",
    "                    skipped_count += 1\n",
    "                    continue\n",
    "                \n",
    "                # Add to processed data\n",
    "                self.processed_data.append({\n",
    "                    \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "                    \"labels\": torch.tensor(labels, dtype=torch.long),\n",
    "                })\n",
    "                \n",
    "                # # Debug first few samples\n",
    "                # if len(self.processed_data) <= 3:\n",
    "                #     logger.info(f\"Sample {len(self.processed_data)}:\")\n",
    "                #     logger.info(f\"  Article length: {len(article_tokens)}\")\n",
    "                #     logger.info(f\"  Summary length: {len(summary_tokens)}\")\n",
    "                #     logger.info(f\"  Total length: {len(input_ids)}\")\n",
    "                #     logger.info(f\"  Active labels: {len(active_labels)}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error processing sample {idx}: {e}\")\n",
    "                skipped_count += 1\n",
    "                continue\n",
    "        \n",
    "        logger.info(\"Dataset processing complete:\")\n",
    "        logger.info(f\"  Original samples: {len(self.dataset)}\")\n",
    "        logger.info(f\"  Processed samples: {len(self.processed_data)}\")\n",
    "        logger.info(f\"  Skipped samples: {skipped_count}\")\n",
    "        \n",
    "        if len(self.processed_data) == 0:\n",
    "            raise ValueError(\"No valid samples found in dataset! Check your data format and tokenization.\")\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.processed_data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(logits, targets):\n",
    "    \"\"\"Calculates loss, accuracy, and perplexity score for a batch.\"\"\"\n",
    "    logits = logits.to(targets.device)\n",
    "    mask = targets != -100\n",
    "\n",
    "    if not mask.any():\n",
    "        return torch.tensor(0.0, device=logits.device, requires_grad=True), torch.tensor(0.0, device=logits.device), torch.tensor(1.0, device=logits.device)\n",
    "\n",
    "    logits_flat = logits.view(-1, logits.size(-1))\n",
    "    targets_flat = targets.view(-1)\n",
    "\n",
    "    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=-100)\n",
    "\n",
    "    active_mask = targets_flat != -100\n",
    "    if active_mask.any():\n",
    "        active_logits = logits_flat[active_mask]\n",
    "        active_targets = targets_flat[active_mask]\n",
    "        predicted_labels = torch.argmax(active_logits, dim=1)\n",
    "        accuracy = (predicted_labels == active_targets).float().mean()\n",
    "    else:\n",
    "        accuracy = torch.tensor(0.0, device=logits.device)\n",
    "\n",
    "    perplexity = torch.exp(torch.clamp(loss, max=10.0))\n",
    "\n",
    "    return loss, accuracy, perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-31 22:21:14.117\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m283\u001b[0m - \u001b[1mTesting basic dataset loading...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:16.586\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m285\u001b[0m - \u001b[1mSuccessfully loaded 18949 training samples\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:16.589\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m289\u001b[0m - \u001b[1mSample keys: dict_keys(['text', 'summary', 'title'])\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:16.590\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m290\u001b[0m - \u001b[1mText length: 5026\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:16.591\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m291\u001b[0m - \u001b[1mSummary length: 1561\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.407\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m295\u001b[0m - \u001b[1mTokenizer loaded, vocab size: 50257\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.409\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m300\u001b[0m - \u001b[1mTest tokenization: 'This is a test.' -> [1212, 318, 257, 1332, 13]\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.410\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m303\u001b[0m - \u001b[1mTesting dataset processing...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.412\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mProcessing and masking dataset for summarization...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.413\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mOriginal dataset size: 5\u001b[0m\n",
      "100%|██████████| 5/5 [00:00<00:00, 291.53it/s]\n",
      "\u001b[32m2025-07-31 22:21:18.434\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset processing complete:\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.435\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Original samples: 5\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.436\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1m  Processed samples: 5\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.437\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1m  Skipped samples: 0\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.438\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m312\u001b[0m - \u001b[1mSuccessfully processed 5 samples\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m316\u001b[0m - \u001b[1mProcessed sample keys: dict_keys(['input_ids', 'labels'])\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.439\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m317\u001b[0m - \u001b[1mInput IDs shape: torch.Size([1024])\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.440\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m318\u001b[0m - \u001b[1mLabels shape: torch.Size([1024])\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.445\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mdebug_dataset_loading\u001b[0m:\u001b[36m319\u001b[0m - \u001b[1mActive labels count: 316\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.446\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m333\u001b[0m - \u001b[1mDataset loading test passed!\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.453\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m51\u001b[0m - \u001b[1mTokenizer vocab size: 50257\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.454\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m52\u001b[0m - \u001b[1mEOT token: 50256\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:18.455\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mLoading datasets...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:19.382\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoaded train set: 13264 samples\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:19.383\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mLoaded val set: 5685 samples\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:19.384\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mProcessing training dataset...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:19.385\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mProcessing and masking dataset for summarization...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:19.386\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mOriginal dataset size: 13264\u001b[0m\n",
      "100%|██████████| 13264/13264 [00:22<00:00, 587.74it/s]\n",
      "\u001b[32m2025-07-31 22:21:41.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset processing complete:\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:41.958\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Original samples: 13264\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:41.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1m  Processed samples: 13218\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:41.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1m  Skipped samples: 46\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:41.959\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing validation dataset...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:41.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mProcessing and masking dataset for summarization...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:41.960\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mOriginal dataset size: 5685\u001b[0m\n",
      "100%|██████████| 5685/5685 [00:07<00:00, 746.63it/s]\n",
      "\u001b[32m2025-07-31 22:21:49.576\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset processing complete:\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:49.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Original samples: 5685\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:49.577\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1m  Processed samples: 5668\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:49.578\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1m  Skipped samples: 17\u001b[0m\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "You have turned on `Trainer(detect_anomaly=True)`. This will significantly slow down compute speed and is recommended only for model debugging.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.21.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>./wandb/run-20250731_222151-m4tv19w1</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/nilesh-auradkar-personal/summarization-gpt2-finetune_V2/runs/m4tv19w1' target=\"_blank\">fine-tune-gpt2-small (124M)-20250731-222118</a></strong> to <a href='https://wandb.ai/nilesh-auradkar-personal/summarization-gpt2-finetune_V2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/nilesh-auradkar-personal/summarization-gpt2-finetune_V2' target=\"_blank\">https://wandb.ai/nilesh-auradkar-personal/summarization-gpt2-finetune_V2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/nilesh-auradkar-personal/summarization-gpt2-finetune_V2/runs/m4tv19w1' target=\"_blank\">https://wandb.ai/nilesh-auradkar-personal/summarization-gpt2-finetune_V2/runs/m4tv19w1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-31 22:21:52.423\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m57\u001b[0m - \u001b[1mLoading datasets...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:53.459\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m66\u001b[0m - \u001b[1mLoaded train set: 13264 samples\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:53.460\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mLoaded val set: 5685 samples\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:53.461\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m75\u001b[0m - \u001b[1mProcessing training dataset...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:53.463\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mProcessing and masking dataset for summarization...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:21:53.464\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mOriginal dataset size: 13264\u001b[0m\n",
      "100%|██████████| 13264/13264 [00:20<00:00, 650.94it/s]\n",
      "\u001b[32m2025-07-31 22:22:13.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset processing complete:\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:13.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Original samples: 13264\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:13.844\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1m  Processed samples: 13218\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:13.845\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1m  Skipped samples: 46\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:13.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m82\u001b[0m - \u001b[1mProcessing validation dataset...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:13.867\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m10\u001b[0m - \u001b[1mProcessing and masking dataset for summarization...\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:13.868\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m11\u001b[0m - \u001b[1mOriginal dataset size: 5685\u001b[0m\n",
      "100%|██████████| 5685/5685 [00:07<00:00, 760.41it/s]\n",
      "\u001b[32m2025-07-31 22:22:21.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m94\u001b[0m - \u001b[1mDataset processing complete:\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:21.346\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m95\u001b[0m - \u001b[1m  Original samples: 5685\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:21.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m96\u001b[0m - \u001b[1m  Processed samples: 5668\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:21.347\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m__init__\u001b[0m:\u001b[36m97\u001b[0m - \u001b[1m  Skipped samples: 17\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:21.366\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m169\u001b[0m - \u001b[1mLoading gpt2-small (124M) model in setup()\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading GPT-2 124M model to ./model_weights/124M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ./model_weights/124M/checkpoint\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ./model_weights/124M/encoder.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ./model_weights/124M/hparams.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ./model_weights/124M/model.ckpt.data-00000-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ./model_weights/124M/model.ckpt.index\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ./model_weights/124M/model.ckpt.meta\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'openaipublic.blob.core.windows.net'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists and is up-to-date: ./model_weights/124M/vocab.bpe\n",
      "Download completed. Loading model parameters...\n",
      "Model loaded successfully!\n",
      "Model configuration: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Settings: {'n_vocab': 50257, 'n_ctx': 1024, 'n_embd': 768, 'n_head': 12, 'n_layer': 12}\n",
      "Params: dict_keys(['blocks', 'b', 'g', 'wpe', 'wte'])\n",
      "['gpt2-small (124M)', 'gpt2-medium (355M)', 'gpt2-large (774M)', 'gpt2-xl (1558M)']\n",
      "Returned Model config for gpt2-small (124M): {'vocab_size': 50257, 'context_length': 1024, 'embedding_dim': 768, 'num_layers': 12, 'num_heads': 12, 'drop_rate': 0.1, 'qkv_bias': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-31 22:22:26.325\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m176\u001b[0m - \u001b[1mModel laoded Successfully! from setup()\u001b[0m\n",
      "\u001b[32m2025-07-31 22:22:26.326\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36msetup\u001b[0m:\u001b[36m181\u001b[0m - \u001b[1mFreezing all parameters of the gpt2-small (124M) model\u001b[0m\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name               | Type                | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0 | summarization_head | SummarizationNNHead | 156 M  | train\n",
      "1 | gpt2_base          | GPT2ModelClone      | 163 M  | eval \n",
      "-------------------------------------------------------------------\n",
      "156 M     Trainable params\n",
      "163 M     Non-trainable params\n",
      "319 M     Total params\n",
      "1,279.357 Total estimated model params size (MB)\n",
      "6         Modules in train mode\n",
      "187       Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-31 22:22:27.248\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mval_dataloader\u001b[0m:\u001b[36m123\u001b[0m - \u001b[1mCreating val dataloader with 5668 samples\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m2025-07-31 22:22:37.754\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain_dataloader\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mCreating train dataloader with 13218 samples\u001b[0m\n",
      "/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=11` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 55/4406 [09:11<12:07:02,  0.10it/s, v_num=19w1, train_loss_step=17.30]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:344\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1328\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1305\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1306\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1326\u001b[39m \n\u001b[32m   1327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/amp.py:79\u001b[39m, in \u001b[36mMixedPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[33m\"\u001b[39m\u001b[33mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m79\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:140\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_backward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep_output\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    142\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m step_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:241\u001b[39m, in \u001b[36m_AutomaticOptimization._make_backward_fn.<locals>.backward_fn\u001b[39m\u001b[34m(loss)\u001b[39m\n\u001b[32m    240\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbackward_fn\u001b[39m(loss: Tensor) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m     \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mbackward\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:213\u001b[39m, in \u001b[36mStrategy.backward\u001b[39m\u001b[34m(self, closure_loss, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m    211\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.pre_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n\u001b[32m--> \u001b[39m\u001b[32m213\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    215\u001b[39m closure_loss = \u001b[38;5;28mself\u001b[39m.precision_plugin.post_backward(closure_loss, \u001b[38;5;28mself\u001b[39m.lightning_module)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/precision.py:73\u001b[39m, in \u001b[36mPrecision.backward\u001b[39m\u001b[34m(self, tensor, model, optimizer, *args, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Performs the actual backpropagation.\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     71\u001b[39m \n\u001b[32m     72\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1097\u001b[39m, in \u001b[36mLightningModule.backward\u001b[39m\u001b[34m(self, loss, *args, **kwargs)\u001b[39m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1097\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 397\u001b[39m\n\u001b[32m    381\u001b[39m memory_logger_callback = MemoryUsageLogger()\n\u001b[32m    383\u001b[39m trainer = pl.Trainer(\n\u001b[32m    384\u001b[39m     accelerator=\u001b[33m\"\u001b[39m\u001b[33mgpu\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    385\u001b[39m     devices=\u001b[32m1\u001b[39m, \u001b[38;5;66;03m# -1 for all available GPUs\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    394\u001b[39m     detect_anomaly=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    395\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    399\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.is_global_zero:\n\u001b[32m    400\u001b[39m     wandb.finish()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/media/cosmic-muffin/wd_black/git/envs/fine-tune-tf/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"This scripts:\n",
    "    1. initializes new nn head for summarization.\n",
    "    2. freezes all pre-train weights.\n",
    "    3. unfreezes last block of transformer.\n",
    "    4. replaces original out head with new nn head for summarization training.\"\"\"\n",
    "\n",
    "# from src.model.prepare_for_fine_tune import PrepareModelWithPreTrainedWeights\n",
    "# from utils.load_dataset import MaskedBillSumDataset\n",
    "# from utils.util import calculate_metrics\n",
    "\n",
    "class SummarizationNNHead(nn.Module):\n",
    "    \"\"\"A Multi-Layer Perceptron Head for Summarization\"\"\"\n",
    "    def __init__(self, embedding_dim, vocab_size, hidden_dim_factor=4):\n",
    "        super().__init__()\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(embedding_dim, embedding_dim * hidden_dim_factor),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(hidden_dim_factor * embedding_dim, vocab_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "class MemoryUsageLogger(Callback):\n",
    "    def on_train_epoch_end(self, trainer, pl_module):\n",
    "        if not trainer.is_global_zero:\n",
    "            return\n",
    "        \n",
    "        ram_stats = psutil.virtual_memory()\n",
    "        ram_used_gb = ram_stats.used / (1024 ** 3)\n",
    "\n",
    "        vram_used_gb = 0\n",
    "        if torch.cuda.is_available():\n",
    "            device_idx = trainer.local_rank\n",
    "            vram_used_gb = torch.cuda.memory_allocated(device_idx) / (1024 ** 3)\n",
    "\n",
    "        metrics = {\"memory/ram_used_gb\": ram_used_gb, \"memory/vram_used_gb\": vram_used_gb}\n",
    "        trainer.logger.log_metrics(metrics, step=trainer.global_step)\n",
    "        logger.info(f\"Memory @ epoch {trainer.current_epoch}: RAM: {ram_used_gb:.2f}GB | VRAM: {vram_used_gb:.2f}GB\")\n",
    "\n",
    "\n",
    "class SummarizationDataModule(pl.LightningDataModule):\n",
    "    \"\"\"Enhanced Data Module with better error handling\"\"\"\n",
    "    def __init__(self, model_config, train_config):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        \n",
    "        # Add validation\n",
    "        logger.info(f\"Tokenizer vocab size: {self.tokenizer.n_vocab}\")\n",
    "        logger.info(f\"EOT token: {self.tokenizer.eot_token}\")\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        try:\n",
    "            # Load datasets with error handling\n",
    "            logger.info(\"Loading datasets...\")\n",
    "            \n",
    "            # Try smaller subset first for testing\n",
    "            if stage == \"fit\" or stage is None:\n",
    "                full_dataset = load_dataset(\"FiscalNote/billsum\", split=\"train\")\n",
    "                split_dataset = full_dataset.train_test_split(test_size=0.3, seed=47)\n",
    "                train_set = split_dataset[\"train\"]\n",
    "                val_set = split_dataset[\"test\"]\n",
    "                \n",
    "                logger.info(f\"Loaded train set: {len(train_set)} samples\")\n",
    "                logger.info(f\"Loaded val set: {len(val_set)} samples\")\n",
    "                \n",
    "                # Smaller subset for debugging\n",
    "                # train_set = train_set.select(range(min(1000, len(train_set))))\n",
    "                # val_set = val_set.select(range(min(200, len(val_set))))\n",
    "                \n",
    "                # Process datasets\n",
    "                logger.info(\"Processing training dataset...\")\n",
    "                self.train_dataset = MaskedBillSumDataset(\n",
    "                    train_set, \n",
    "                    self.tokenizer, \n",
    "                    self.hparams.model_config['context_length']\n",
    "                )\n",
    "                \n",
    "                logger.info(\"Processing validation dataset...\")\n",
    "                self.val_dataset = MaskedBillSumDataset(\n",
    "                    val_set, \n",
    "                    self.tokenizer, \n",
    "                    self.hparams.model_config['context_length']\n",
    "                )\n",
    "            \n",
    "            # Test dataset for later use\n",
    "            if stage == \"test\" or stage is None:\n",
    "                test_set = load_dataset(\"FiscalNote/billsum\", split=\"ca_test\")\n",
    "                logger.info(f\"Loaded test set: {len(test_set)} samples\")\n",
    "                \n",
    "                self.test_dataset = MaskedBillSumDataset(\n",
    "                    test_set, \n",
    "                    self.tokenizer, \n",
    "                    self.hparams.model_config['context_length']\n",
    "                )\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error in setup: {e}\")\n",
    "            raise\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        if not hasattr(self, 'train_dataset') or len(self.train_dataset) == 0:\n",
    "            raise ValueError(\"Training dataset is empty or not initialized!\")\n",
    "        \n",
    "        logger.info(f\"Creating train dataloader with {len(self.train_dataset)} samples\")\n",
    "        \n",
    "        return DataLoader(\n",
    "            self.train_dataset, \n",
    "            batch_size=self.hparams.train_config[\"batch_size\"], \n",
    "            num_workers=0, \n",
    "            drop_last=True, \n",
    "            shuffle=True,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        if not hasattr(self, 'val_dataset') or len(self.val_dataset) == 0:\n",
    "            raise ValueError(\"Validation dataset is empty or not initialized!\")\n",
    "        \n",
    "        logger.info(f\"Creating val dataloader with {len(self.val_dataset)} samples\")\n",
    "        \n",
    "        return DataLoader(\n",
    "            self.val_dataset, \n",
    "            batch_size=self.hparams.train_config[\"batch_size\"], \n",
    "            num_workers=5, \n",
    "            drop_last=False, \n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "        if not hasattr(self, 'test_dataset') or len(self.test_dataset) == 0:\n",
    "            raise ValueError(\"Test dataset is empty or not initialized!\")\n",
    "        \n",
    "        return DataLoader(\n",
    "            self.test_dataset, \n",
    "            batch_size=self.hparams.train_config[\"batch_size\"], \n",
    "            num_workers=5, \n",
    "            drop_last=False, \n",
    "            shuffle=False,\n",
    "            pin_memory=True\n",
    "        )\n",
    "    \n",
    "class SummarizationFineTuneModel(pl.LightningModule):\n",
    "    def __init__(self, model_name, model_config, train_config, num_training_steps):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.gpt2_base = None\n",
    "        self.model_loaded = False\n",
    "\n",
    "        self.summarization_head = SummarizationNNHead(\n",
    "            embedding_dim=self.hparams.model_config['embedding_dim'],\n",
    "            vocab_size=self.hparams.model_config['vocab_size']\n",
    "        )\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        if not self.model_loaded:\n",
    "            logger.info(f\"Loading {self.hparams.model_name} model in setup()\")\n",
    "            model_loader = PrepareModelWithPreTrainedWeights(\n",
    "                model_name=self.hparams.model_name,\n",
    "                device=\"cpu\"\n",
    "            )\n",
    "            if model_loader.model is not None:\n",
    "                self.gpt2_base = model_loader.model\n",
    "                logger.info(\"Model laoded Successfully! from setup()\")\n",
    "            else:\n",
    "                logger.error(\"Model loading failed from setup()\")\n",
    "                raise ValueError(\"Model loading failed from setup()\")\n",
    "\n",
    "            logger.info(f\"Freezing all parameters of the {self.hparams.model_name} model\")\n",
    "            for param in self.gpt2_base.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "            self.model_loaded = True\n",
    "\n",
    "    def _init_weights(self):\n",
    "        \"\"\"Initializing weights with small values to prevent instability in training.\"\"\"\n",
    "        for module in self.summarization_head.modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "                if module.bias is not None:\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        in_idx = batch['input_ids']\n",
    "\n",
    "        # Input validation\n",
    "        if torch.isnan(in_idx).any() or torch.isinf(in_idx).any():\n",
    "            raise ValueError(\"Invalid input tokens detected\")\n",
    "        \n",
    "        if self.gpt2_base is None:\n",
    "            raise ValueError(\"GPT-2 model not loaded.\")\n",
    "\n",
    "        with torch.no_grad():        \n",
    "            tok_embeds = self.gpt2_base.tok_embeddings(in_idx)\n",
    "            pos_embeds = self.gpt2_base.pos_embeddings(torch.arange(in_idx.shape[1], device=self.device))\n",
    "            input_embeds = tok_embeds + pos_embeds\n",
    "            x = self.gpt2_base.drop_embeddings(input_embeds)\n",
    "            x = self.gpt2_base.transformer_blocks(x)\n",
    "            hidden_states = self.gpt2_base.final_norm(x)\n",
    "\n",
    "        logits = self.summarization_head(hidden_states)\n",
    "\n",
    "        # Checking inf/nan in outputs\n",
    "        if torch.isnan(logits).any() or torch.isinf(logits).any():\n",
    "            logger.warning(\"NaN or Inf detected in logits\")\n",
    "            return torch.zeros_like(logits)\n",
    "            \n",
    "        return logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # # Debugging\n",
    "        # if batch_idx == 0:\n",
    "        #     logger.info(f\"Batch input_ids shape: {batch['input_ids'].shape}\")\n",
    "        #     logger.info(f\"Batch labels shape: {batch['labels'].shape}\")\n",
    "        #     logger.info(f\"Active labels count: {(batch['labels'] != -100).sum()}\")\n",
    "            \n",
    "        logits = self(batch)\n",
    "        loss, acc, perplexity = calculate_metrics(logits, batch['labels'])\n",
    "\n",
    "        # NaN Checking\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            logger.warning(f\"NaN/Inf loss detected at step {batch_idx}\")\n",
    "            return None\n",
    "        \n",
    "        self.log('train_loss', loss, on_step=True, on_epoch=True,\n",
    "                 prog_bar=True, logger=True, sync_dist=True)\n",
    "        self.log('train_acc', acc, on_step=True, on_epoch=True,\n",
    "                 logger=True, sync_dist=True)\n",
    "        self.log(\"train_perplexity\", perplexity, on_epoch=True, logger=True, sync_dist=True)\n",
    "        # print(f\"Train Loss: {loss} | Train Acc: {acc} | Train Perplexity Score: {perplexity}\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        logits = self(batch)\n",
    "        loss, acc, perplexity = calculate_metrics(logits, batch['labels'])\n",
    "\n",
    "        if torch.isnan(loss) or torch.isinf(loss):\n",
    "            logger.warning(f\"NaN/Inf validation loss detected at step {batch_idx}. Skipping...\")\n",
    "            return None\n",
    "\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True,\n",
    "                 logger=True, sync_dist=True)\n",
    "        self.log('val_acc', acc, on_epoch=True, logger=True, sync_dist=True)\n",
    "        self.log('val_perplexity', perplexity, on_epoch=True, logger=True, sync_dist=True)\n",
    "        # print(f\"Val loss: {loss} | Val Acc: {acc} | Val Perplexity Score: {perplexity}\")\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.summarization_head.parameters(),\n",
    "            lr=self.hparams.train_config['learning_rate'],\n",
    "            weight_decay=0.01,\n",
    "            eps=1e-8\n",
    "        )\n",
    "        num_training_steps = self.hparams.num_training_steps\n",
    "        num_warmup_steps = int(0.1 * num_training_steps)\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "        return {'optimizer': optimizer, 'lr_scheduler': {'scheduler': scheduler, 'interval': 'step'}}\n",
    "\n",
    "# Debug function to test dataset loading\n",
    "def debug_dataset_loading():\n",
    "    \"\"\"Function to debug dataset loading issues\"\"\"\n",
    "    \n",
    "    # Test basic dataset loading\n",
    "    try:\n",
    "        logger.info(\"Testing basic dataset loading...\")\n",
    "        train_set = load_dataset(\"FiscalNote/billsum\", split=\"train\")\n",
    "        logger.info(f\"Successfully loaded {len(train_set)} training samples\")\n",
    "        \n",
    "        # Check first sample\n",
    "        sample = train_set[0]\n",
    "        logger.info(f\"Sample keys: {sample.keys()}\")\n",
    "        logger.info(f\"Text length: {len(sample.get('text', ''))}\")\n",
    "        logger.info(f\"Summary length: {len(sample.get('summary', ''))}\")\n",
    "        \n",
    "        # Test tokenizer\n",
    "        tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "        logger.info(f\"Tokenizer loaded, vocab size: {tokenizer.n_vocab}\")\n",
    "        \n",
    "        # Test tokenization\n",
    "        test_text = \"This is a test.\"\n",
    "        tokens = tokenizer.encode(test_text)\n",
    "        logger.info(f\"Test tokenization: '{test_text}' -> {tokens}\")\n",
    "        \n",
    "        # Test dataset processing with just first sample\n",
    "        logger.info(\"Testing dataset processing...\")\n",
    "        small_dataset = train_set.select(range(5))  # Just 5 samples\n",
    "        \n",
    "        processed_dataset = MaskedBillSumDataset(\n",
    "            small_dataset,\n",
    "            tokenizer,\n",
    "            max_length=1024\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Successfully processed {len(processed_dataset)} samples\")\n",
    "        \n",
    "        if len(processed_dataset) > 0:\n",
    "            sample = processed_dataset[0]\n",
    "            logger.info(f\"Processed sample keys: {sample.keys()}\")\n",
    "            logger.info(f\"Input IDs shape: {sample['input_ids'].shape}\")\n",
    "            logger.info(f\"Labels shape: {sample['labels'].shape}\")\n",
    "            logger.info(f\"Active labels count: {(sample['labels'] != -100).sum()}\")\n",
    "            \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Dataset loading test failed: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Load Model Configs\n",
    "\n",
    "    if debug_dataset_loading():\n",
    "        logger.info(\"Dataset loading test passed!\")\n",
    "    else:\n",
    "        logger.error(\"Dataset loading test failed!\")\n",
    "        exit(1)\n",
    "\n",
    "    with open(\"../config/training_config.yaml\", \"r\") as f:\n",
    "        train_config_full = yaml.safe_load(f)\n",
    "\n",
    "    with open(\"../config/model_config.yaml\", \"r\") as f:\n",
    "        model_config_full = yaml.safe_load(f)\n",
    "\n",
    "    # Extract model and training config\n",
    "    model_name = \"gpt2-small (124M)\"\n",
    "    model_config = model_config_full[\"model_configs\"][model_name]\n",
    "    train_config = train_config_full[\"train_config\"]\n",
    "    wandb_config = train_config_full[\"wandb_config\"]\n",
    "    experiment_path = train_config_full[\"experiment_path\"]\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    wandb_project_name = wandb_config[\"project\"] + str(\"_V2\")\n",
    "    wandb_run_name = f\"fine-tune-{model_name}-{timestamp}\"\n",
    "\n",
    "\n",
    "    os.makedirs(os.path.join(experiment_path, \"checkpoints\"), exist_ok=True)\n",
    "\n",
    "    # Initialize Data Module and Lightning Module\n",
    "    data_module = SummarizationDataModule(model_config, train_config)\n",
    "    data_module.setup(stage=\"fit\")\n",
    "\n",
    "    num_training_steps = (len(data_module.train_dataset) // train_config[\"batch_size\"]) * train_config[\"num_epochs\"]\n",
    "    model_module = SummarizationFineTuneModel(model_name, model_config, train_config, num_training_steps)\n",
    "\n",
    "    wandb_logger = WandbLogger(\n",
    "        name=wandb_run_name,\n",
    "        project=wandb_project_name,\n",
    "        log_model=\"all\",\n",
    "        config={\"model_config\": model_config, \"train_config\": train_config},\n",
    "    )\n",
    "\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        dirpath=os.path.join(experiment_path, \"checkpoints\"),\n",
    "        filename=\"summarization-gpt2-finetune-Epoch-{epoch:02d}-val_loss-{val_loss:.2f}\",\n",
    "        save_top_k=3,\n",
    "        monitor=\"val_loss\",\n",
    "        mode=\"min\",\n",
    "    )\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", patience=3, verbose=True, mode=\"min\")\n",
    "    memory_logger_callback = MemoryUsageLogger()\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        accelerator=\"gpu\",\n",
    "        devices=1, # -1 for all available GPUs\n",
    "        # strategy=\"ddp_notebook\",\n",
    "        max_epochs=train_config[\"num_epochs\"],\n",
    "        logger=wandb_logger,\n",
    "        callbacks=[checkpoint_callback, memory_logger_callback, early_stop_callback],\n",
    "        gradient_clip_val=1.0,\n",
    "        gradient_clip_algorithm=\"norm\",\n",
    "        accumulate_grad_batches=1,\n",
    "        precision=\"16-mixed\",\n",
    "        detect_anomaly=True,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model_module, datamodule=data_module)\n",
    "\n",
    "    if trainer.is_global_zero:\n",
    "        wandb.finish()\n",
    "\n",
    "    print(\"Training complete!\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fine-tune-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
